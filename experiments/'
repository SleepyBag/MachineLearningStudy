from mxnet import gluon
from mxnet import ndarray as nd
from mxnet import autograd
from mxnet.gluon import data as gdata
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import pyplot as plt

net = gluon.nn.Sequential()
with net.name_scope():
    net.add(gluon.nn.Dense(256, activation="relu"))
    net.add(gluon.nn.Dense(10))
net.initialize()

batch_size = 256

softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()
trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.5})
X = nd.random_uniform(-100, 100, shape=(1000))
Y = nd.random_uniform(-100, 100, shape=(1000))
XY = nd.concat(X.reshape((-1, 1)), Y.reshape((-1, 1)), dim=1)
Z = X * Y



net = gluon.nn.Sequential()
net.add(gluon.nn.Dense(10), activation='relu')
net.add(gluon.nn.Dense(10), activation='relu')
net.add(gluon.nn.Dense(1), activation='relu')

batch_size = 1000
dataset = gdata.ArrayDataset(XY, Z)
dataiter = gdata.DataLoader(dataset, batch_size=batch_size)
trainer = gluon.Trainer(net.collect_params(), 'adam',
                        {'beta1': .9, 'beta2': .999})

for epoch in range(5):
    train_loss = 0.
    train_acc = 0.
    for data, label in dataiter:
        with autograd.record():
            output = net(data)
            loss = softmax_cross_entropy(output, label)
        loss.backward()
        trainer.step(batch_size)

        train_loss += nd.mean(loss).asscalar()


figure = plt.figure()
axes = Axes3D(figure)
axes.plot_surface(X.asnumpy(), Y.asnumpy(), XY.asnumpy())
