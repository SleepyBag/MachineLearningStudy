# Neural Collaborative Filtering

这篇文章通过神经网络对SVD模型的点乘计算相关度的方案进行了拓展,产生了NCF模型,用以学习更加复杂的用户-商品交互作用.

## SVD的不足
SVD存在的一个主要问题是使用简单的点乘运算来对用户向量p_u与商品向量q_i进行相关度计算,而二者的现实关联可能比这个模型要复杂.
文章中对此给出了一个简单的例子:给定兴趣矩阵R,R中一行代表一位用户,一列代表一个商品.R中为1的位表示用户对商品感兴趣.文中有四位用户,五个商品.我们可以直接使用五维行向量之间的汉明距离来表示用户之间的差异.然而,在进行奇异值分解将用户信息降至二维之后,根据点乘运算,我们无法在二维空间构造一种能还原这四个用户之间关联度大小的布局.

因此,如何构造一个更好的函数g(p,q)来代替SVD模型中的f(p,q)=pq,以描述更复杂的交互作用就成了要解决的问题.
> 照着数据升维的思路,我尝试了一下将SVD中的內积替换为核函数K(p,q).由于机器性能太低,我只尝试了sigma为1的高斯核函数,但是结果并没有什么提升.
> 用核函数替换问题在于核函数本身形式的选择与核函数的超参数调节.在缺乏经验的情况下人工选择核函数与超参数是一件很困难的事情.所以我认为神经网络的作用可以看做是让模型自主学习一个适当的核函数的拟合,从而避免了选择核函数的麻烦.于是我又实验了多层感知机对常用核函数的拟合,发现都可以收敛至不错的结果.

## NCF模型
NCF模型最重要的思路就是将原SVD的点乘过程替换为了MLP网络来学习一个更合适的函数.另外作者还做了一些别的工作.
* 将网络划分为左右两部分.左边保留SVD的原模型,即计算两向量的叉积;右边使用MLP学习一个新的模型.
> 感觉这类似于ResNet的思路:保留一个性能已经足够好的模型,然后同时添加一个模型与之并行,来拟合残差.不过这个网络中二者的结合方式是拼接而不是相加.
* 预训练:先分别预训练左边的点乘模型与右边的MLP模型.分别达到收敛后,再以此时的参数为初始参数对合并的模型进行训练.

## 收获
* 神经网络的重要作用是可以为参数学习一个恰当的函数关系,在我们对某些事物的交互作用不清楚时,也可以尝试用神经网络来对现有模型进行推广.
* 表现足够好的旧模型可以与新模型进行融合.尝试用新模型去学习旧模型的残差.
* 集成模型可以对每个子模型先进行预训练,在此基础上再合并学习.
